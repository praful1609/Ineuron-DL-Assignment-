{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dbedf53",
   "metadata": {},
   "source": [
    "1. **Logistic Regression vs. Perceptron:**\n",
    "   - Logistic Regression is generally preferred over the classical Perceptron for several reasons:\n",
    "     - Logistic Regression provides continuous probabilistic outputs, making it suitable for classification tasks with well-calibrated probabilities.\n",
    "     - The Perceptron only produces binary outputs, which lack probabilistic interpretation.\n",
    "   - To make a Perceptron equivalent to a Logistic Regression classifier, you can:\n",
    "     - Replace the step function (threshold activation) with the logistic (sigmoid) activation function.\n",
    "     - Use a training algorithm like gradient descent with cross-entropy loss instead of the Perceptron training algorithm.\n",
    "\n",
    "2. **Logistic Activation in MLPs:**\n",
    "   - The logistic (sigmoid) activation function was crucial in training the first MLPs because it allowed for smooth, differentiable activation that facilitates gradient-based optimization techniques like backpropagation.\n",
    "   - The smoothness of the logistic function enables gradient information to flow backward through the network during training, making it possible to adjust weights and biases effectively.\n",
    "\n",
    "3. **Popular Activation Functions:**\n",
    "   - Three popular activation functions are:\n",
    "     1. **Sigmoid Function:**\n",
    "        - Formula: Ïƒ(x) = 1 / (1 + e^(-x))\n",
    "     2. **ReLU (Rectified Linear Unit) Function:**\n",
    "        - Formula: ReLU(x) = max(0, x)\n",
    "     3. **Tanh (Hyperbolic Tangent) Function:**\n",
    "        - Formula: tanh(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))\n",
    "   - Drawing them requires plotting their respective curves, which can be done using libraries like Matplotlib.\n",
    "\n",
    "4. **Shapes in MLP:**\n",
    "   - Shape of Input Matrix X: (batch_size, num_inputs)\n",
    "   - Shape of Hidden Layer's Weight Vector Wh: (num_inputs, num_hidden_neurons)\n",
    "   - Shape of Hidden Layer's Bias Vector bh: (num_hidden_neurons,)\n",
    "   - Shape of Output Layer's Weight Vector Wo: (num_hidden_neurons, num_output_neurons)\n",
    "   - Shape of Output Layer's Bias Vector bo: (num_output_neurons,)\n",
    "   - Shape of Network's Output Matrix Y: (batch_size, num_output_neurons)\n",
    "   - Equation for Y: Y = ReLU(X * Wh + bh) * Wo + bo\n",
    "\n",
    "5. **Output Layer Neurons for Email Spam Classification and MNIST:**\n",
    "   - For email spam classification (binary classification), you need one neuron in the output layer with a sigmoid activation function.\n",
    "   - For MNIST (multi-class classification with 10 classes), you need 10 neurons in the output layer with a softmax activation function.\n",
    "\n",
    "6. **Backpropagation vs. Reverse-Mode Autodiff:**\n",
    "   - Backpropagation is a supervised learning algorithm for training neural networks by computing gradients of the loss function with respect to model parameters. It involves forward and backward passes.\n",
    "   - Reverse-Mode Autodiff is a technique for efficiently computing gradients in computational graphs. It is used in backpropagation for the reverse pass, where gradients are computed efficiently.\n",
    "\n",
    "7. **Hyperparameters in MLP:**\n",
    "   - Learning Rate\n",
    "   - Number of Hidden Layers\n",
    "   - Number of Neurons in Each Layer\n",
    "   - Activation Functions\n",
    "   - Batch Size\n",
    "   - Weight Initialization\n",
    "   - Regularization (e.g., Dropout)\n",
    "   - Optimizer (e.g., SGD, Adam)\n",
    "   - Loss Function\n",
    "   - Number of Epochs\n",
    "   - Early Stopping\n",
    "   - Learning Rate Schedule\n",
    "\n",
    "   If the MLP overfits, you can:\n",
    "   - Decrease the model complexity (reduce the number of neurons or layers).\n",
    "   - Apply regularization techniques (e.g., dropout).\n",
    "   - Increase the amount of training data.\n",
    "   - Adjust learning rate and learning rate schedule.\n",
    "   - Experiment with different architectures and hyperparameters.\n",
    "\n",
    "8. **Training Deep MLP on MNIST:**\n",
    "   - Training a deep MLP on MNIST with all the mentioned features requires significant code and resources. It's typically done using frameworks like TensorFlow or PyTorch with proper data loading, model definition, optimization, and monitoring. It involves multiple steps, and the full code is too extensive to provide here. You can refer to online tutorials and documentation for detailed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619376a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
