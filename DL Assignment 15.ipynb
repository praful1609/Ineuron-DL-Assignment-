{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84776808",
   "metadata": {},
   "source": [
    "1. **Initialization with Same Value and He Initialization:**\n",
    "   - Initializing all weights to the same value, even when randomly selected, is generally not recommended because it can lead to symmetry issues during training. Each neuron should have slightly different initial weights to break the symmetry.\n",
    "   - He initialization is designed to initialize weights with values that help mitigate the vanishing/exploding gradient problem and is based on random initialization, not uniform initialization with the same value.\n",
    "\n",
    "2. **Initializing Bias Terms to 0:**\n",
    "   - Initializing bias terms to 0 is a common practice and often works well. It is not problematic as long as the weights are initialized appropriately. However, if you initialize all weights to 0, it can lead to symmetry problems similar to initializing all weights to the same value.\n",
    "\n",
    "3. **Advantages of ELU over ReLU:**\n",
    "   - ELU (Exponential Linear Unit) offers several advantages over ReLU (Rectified Linear Unit):\n",
    "     1. **Handles Vanishing Gradients:** ELU has non-zero gradients for negative inputs, which helps mitigate vanishing gradient issues during training.\n",
    "     2. **Smooth Activation:** ELU is smooth and differentiable everywhere, making it suitable for gradient-based optimization methods.\n",
    "     3. **Robust to Noisy Inputs:** ELU can better handle noisy inputs as it does not output exactly zero for negative inputs, reducing the risk of dead neurons.\n",
    "\n",
    "4. **Use Cases for Activation Functions:**\n",
    "   - ELU: Generally a good default choice due to its advantages mentioned above.\n",
    "   - Leaky ReLU (and variants like Parametric Leaky ReLU): Suitable when you want to prevent dead neurons and benefit from faster convergence compared to traditional ReLU.\n",
    "   - ReLU: Suitable for most cases but may suffer from dying ReLU problem for some neurons.\n",
    "   - Tanh: Suitable for hidden layers in feedforward networks.\n",
    "   - Logistic (Sigmoid): Suitable for binary classification in the output layer.\n",
    "   - Softmax: Suitable for multi-class classification in the output layer.\n",
    "\n",
    "5. **Setting Momentum Hyperparameter too Close to 1:**\n",
    "   - Setting the momentum hyperparameter very close to 1 (e.g., 0.99999) in a MomentumOptimizer can lead to slow convergence and erratic behavior during training. The optimizer accumulates previous gradients to a high degree, making updates excessively reliant on past gradients. This can result in slow convergence or divergence.\n",
    "\n",
    "6. **Producing Sparse Models:**\n",
    "   - Sparse models can be beneficial for reducing memory and computation requirements. Ways to produce sparse models include:\n",
    "     1. **Weight Pruning:** Removing small-weight connections or neurons during or after training.\n",
    "     2. **Regularization:** Applying L1 regularization encourages weights to become sparse by pushing many of them toward zero.\n",
    "     3. **Sparse Activations:** Using activation functions like ReLU can naturally produce sparse activations as they set negative values to zero.\n",
    "\n",
    "7. **Dropout Impact on Training and Inference:**\n",
    "   - Dropout can slow down training because it involves randomly deactivating neurons during each training step, effectively reducing the effective network size.\n",
    "   - During inference (making predictions on new instances), dropout is typically turned off, so it does not slow down the inference process. In fact, it can be seen as an ensemble technique during training, which often improves model generalization without affecting inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e27ead8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
